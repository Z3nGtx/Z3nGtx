import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.preprocessing import LabelEncoder
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.metrics.cluster import adjusted_rand_score
from sklearn.cluster import AgglomerativeClustering

# Load dataset
df = pd.read_csv("Mall_Customers.csv")
print(df.head())
print("Data Shape:", df.shape)
print("Columns:", df.columns)

# Drop the 'CustomerID' column
df.drop("CustomerID", axis=1, inplace=True)

# Check for missing values
print("Missing values:\n", df.isnull().sum())

# Encode the 'Genre' column
le = LabelEncoder()
df["Genre"] = le.fit_transform(df["Genre"])

# Finding the optimum number of clusters using the elbow method
data = df.copy()
x = data.iloc[:, [2, 3]]  # Using 'Annual Income' and 'Spending Score' for clustering
wcss = []

for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)
    kmeans.fit(x)
    wcss.append(kmeans.inertia_)
    print('k:', i, "-> WCSS:", kmeans.inertia_)

# Plotting the results onto a line graph to observe 'The elbow'
plt.plot(range(1, 11), wcss, marker='o')
plt.title('The Elbow Method')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS')
plt.show()

# Choosing k=5 based on the elbow method
km1 = KMeans(n_clusters=5, random_state=42)
km1.fit(x)
pred_y = km1.predict(x)
data["label"] = pred_y

# Scatterplot of the clusters
plt.figure(figsize=(6, 4))
sns.scatterplot(
    x='Annual Income (k$)', y='Spending Score (1-100)', hue="label",
    palette=['green', 'brown', 'orange', 'red', 'dodgerblue'], data=data
)
plt.xlabel('Annual Income (k$)')
plt.ylabel('Spending Score (1-100)')
plt.title('Spending Score (1-100) vs Annual Income (k$)')
plt.show()

# Prepare data for model evaluation
X = df.iloc[:, :4]  # Features: 'Genre', 'Age', 'Annual Income', 'Spending Score'
y = data["label"]   # Cluster labels as target

# Split the dataset into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print("Train Set Shape:", X_train.shape, y_train.shape)
print("Test Set Shape:", X_test.shape, y_test.shape)

# KMeans clustering on train and test sets
km = KMeans(n_clusters=5, random_state=42)
km.fit(X_train)

# Predict clusters on training and test data
y_train_km = km.predict(X_train)
y_test_km = km.predict(X_test)

# Calculate accuracy using adjusted rand score
acc_train_kmeans = adjusted_rand_score(y_train, y_train_km)
acc_test_kmeans = adjusted_rand_score(y_test, y_test_km)

print("KMeans: Accuracy on Training Data: {:.3f}".format(acc_train_kmeans))
print("KMeans: Accuracy on Test Data: {:.3f}".format(acc_test_kmeans))

# Using Agglomerative Clustering on 'Annual Income' and 'Spending Score'
agc = AgglomerativeClustering(n_clusters=5)
data["label"] = agc.fit_predict(x)

# Scatterplot of Agglomerative Clustering results
plt.figure(figsize=(6, 4))
sns.scatterplot(
    x='Annual Income (k$)', y='Spending Score (1-100)', hue="label",
    palette=['green', 'brown', 'orange', 'red', 'dodgerblue'], data=data
)
plt.xlabel('Annual Income (k$)')
plt.ylabel('Spending Score (1-100)')
plt.title('Agglomerative Clustering: Spending Score (1-100) vs Annual Income (k$)')
plt.show()
